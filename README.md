# ensemble-vs-neural-networks
compared machine learning and deep learning techniques with Ensemble Learning techniques. The ensemble techniques are based on the idea of combining predictions from multiple learners, thereby helping in reducing the bias/variance of the machine learning models. implemented all of these base machine learning and deep learning models from scratch. Also implemented bagging technique (Random Forest) using Decision Trees from scratch and have also compared the results with the sklearn implementation of Random Forest as well. For boosting, I’ve taken the implementation from sklearn where I’ve trained the adaboost and gradient boosting models on the three datasets. These datasets (in the increasing order of difficulty are): Iris Dataset, Kr-vs-Kp Dataset and Diabetes Dataset. We’ve also done EDA and visualization of these datasets for better understanding. The general trend that I’ve observed is that Random Forest, an ensemble learning model performs the best, followed by neural network (deep learning method) and adaboost (ensemble learning method). Through these experiments, we were able to understand and implement various machine learning algorithms and also understood how ensemble learning techniques helps in improving the model performance by reducing the bias/variance of a machine learning model.
